{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание №5\n",
    "### по практикуму на ЭВМ (2015-2016)\n",
    "#### Выполнила студентка 317 группы\n",
    "#### Шолохова Татьяна\n",
    "##### 24.02.2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании требовалось реализовать обучение разреженного автокодировщика и показать, как он обнаруживает, что границы объектов и цветовые переходы - одно из лучших представлений для естественных изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "import math\n",
    "import scipy.optimize \n",
    "import display_layer as dl\n",
    "import sample_patches as sp\n",
    "import autoencoder as aec\n",
    "import gradient as g\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Генерация выборки патчей на основе неразмеченных исходных изображений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(sp)\n",
    "def make_patches(num=10000, size=8):\n",
    "    patches = np.zeros((num, size * size * 3))\n",
    "    N = num\n",
    "    k = 5\n",
    "    i = 0\n",
    "    for fname in 'X1', 'X2', 'X3', 'X4', 'X5':\n",
    "        v = (N + k - 1) // k\n",
    "        f = open('./data2.7/' + fname + '.pk', 'rb')\n",
    "        images = pickle.load(f)\n",
    "        patches[i:i + v] = sp.sample_patches(images, v, size)\n",
    "        del images\n",
    "        f.close()\n",
    "        i += v\n",
    "        N -= v\n",
    "        k -= 1\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Исходные изображения**\n",
    "\n",
    "<img src=\"images.png\", width=200, align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(dl)\n",
    "reload(sp)\n",
    "data = make_patches()\n",
    "dl.display_layer(data[:64], 'patches.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сгенерированные патчи**\n",
    "\n",
    "<img src=\"patches.png\", width=200, align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Формулы для подсчета градиентов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь является суммой трёх слагаемых: квадратичное отклонение исходных изображений $X$ от выходных $Y$; регуляризация, ограничивающая веса коэффициентов; регуляризация разреженности среднего слоя. \n",
    "$$L = Q + R_1 + R_2$$\n",
    "$$Q = \\frac{1}{2N}||X-Y||_F^2$$\n",
    "$$R_1 = \\frac{\\lambda}{2}\\sum\\limits_{i=0}^{l-1}||W^{(i)}||_F^2$$\n",
    "$$R_2 = \\beta\\sum\\limits_{i = 1}^{s}\\left(\\rho \\log\\frac{\\rho}{\\hat\\rho_i} + (1 - \\rho) \\log\\frac{1 - \\rho}{1 - \\hat\\rho_i}\\right)$$ , где $N$ &ndash; количество изображений, $l$ &ndash; количество слоёв нейросети, $||\\cdot||_F$ &ndash; норма Фробениуса, $\\lambda, \\rho, \\beta$ &ndash; параметры регуляризаций, $W^{(i)}$ &ndash; матрица весов между $i$ и $i+1$ слоями, $s$ &ndash; количество нейронов на среднем слое, $\\hat\\rho_j$ &ndash; среднее значение активации $j$-го нейрона на среднем слое. \n",
    "\n",
    "Если бы функция потерь состояла только из квадратичного отклонения исходных изображений $X$ от выходных $Y$, то градиент этой функции по параметрам нейросети можно вычислить, используя метод обратного распространения ошибки(соответствующие формулы приведены в условии задания).\n",
    "\n",
    "Выясним как изменится градиент при добавлении только первого регуляризатора. \n",
    "$$\\frac{\\partial L}{\\partial W^{(i)}} = \\frac{\\partial Q}{\\partial W^{(i)}} + \\frac{\\partial R_1}{\\partial W^{(i)}}$$\n",
    "Первое слагаемое мы уже умеем вычислять, второе слагаемое можно выписать и вычислить аналитически:\n",
    "$$\\frac{\\partial R_1}{\\partial W^{(i)}} = \\lambda W^{(i)}$$\n",
    "Для корректного вычисления градиентов в этом случае, необходимо выполнить алгоритм обратного распространения ошибки по \"старым\" формулам, после чего добавить к градиентам по всем $W^{(i)}$ вычисленную добавку $\\lambda W^{(i)}$.\n",
    "\n",
    "Теперь добавим второй регуляризатор. Обозначим $S$ матрицу активаций по всем изображениям на среднем слое автокодировщика(матрица $N \\times D$, $N$ &ndash; количество изображений, $D$ &ndash; размерность среднего слоя).\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial Q}{\\partial A} + \\frac{\\partial R_2}{\\partial A}$$\n",
    "Первое слагаемое мы уже умеем вычислять, второе слагаемое:\n",
    "$$\\frac{\\partial R_2}{\\partial A_{ij}} = \\sum \\limits_{k = 1}^{s} \\frac{\\partial R_2}{\\partial \\hat\\rho_k} \\frac{\\partial \\hat\\rho_k}{\\partial A_{ij}}$$\n",
    "$$\\frac{\\partial R_2}{\\partial \\hat\\rho_k} = \\beta\\left(-\\frac{\\hat\\rho_k}{\\rho} + \\frac{1 - \\hat\\rho_k}{1 - \\rho}\\right)$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat\\rho_k}{\\partial A_{ij}} = \n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "    0,& \\qquad k \\neq j \\\\\n",
    "    \\frac{1}{N},& \\qquad k = j \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial R_2}{\\partial A_{ij}} = \\frac{\\beta}{N} \\left(-\\frac{\\hat\\rho_j}{\\rho} + \\frac{1 - \\hat\\rho_j}{1 - \\rho} \\right)\n",
    "$$\n",
    "\n",
    "Тем самым для корректного учета градиента по $R_2$ в методе обратного распространения ошибки необходимо прибавить выписанную добавку при вычислении $\\frac{\\partial L}{\\partial A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение разреженного автокодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка корректности функции приближенного вычисления градиента на основе разностной апроксимации. Объявленная простая функция:\n",
    "$$J(\\vec{x}) = \\sin{(x_0 x_1 x_2)} + x_0 x_2 $$, легко убедиться, что ее градиент $$\\frac{\\partial{L}}{\\partial{\\vec{x}}} = (x_1 x_2\\cos{(x_0 x_1 x_2)} + x_2~~ ,~~ x_2 x_0 \\cos{(x_0 x_1 x_2)}~~ ,~~ x_0 x_1 \\cos{(x_0 x_1 x_2)} + x_0)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "reload(g)\n",
    "g.check_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Погрешность не более $10^{-4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Вычисление функции потерь разреженного однослойного автокодировщика и соответствующего градиента с сигмоидной функцией активации и сравнение полученного градиента с приближенно вычисленным **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.40606782761e-09\n"
     ]
    }
   ],
   "source": [
    "reload(aec)\n",
    "hidden_size = np.array([75])\n",
    "visible_size = 192\n",
    "lambda_ = 1e-4\n",
    "sparsity_param = 1e-2\n",
    "beta = 1e-1\n",
    "theta = aec.initialize(hidden_size, visible_size)\n",
    "grad = aec.autoencoder_loss(theta, visible_size, hidden_size, lambda_, \\\n",
    "                            sparsity_param, beta, data[:10])[1]\n",
    "reload(g)\n",
    "J = lambda theta: aec.autoencoder_loss(theta, visible_size, hidden_size, \\\n",
    "                                       lambda_, sparsity_param, beta, data[:10])[0]\n",
    "True_grad = g.compute_gradient(J, theta)\n",
    "print(np.max(np.abs(True_grad - grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Минимизация функции потерь разреженного однослойного автокодировщика**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(aec)\n",
    "hidden_size = np.array([75])\n",
    "visible_size = 192\n",
    "lambda_ = 5e-4\n",
    "sparsity_param = 1e-2\n",
    "beta = 3\n",
    "theta = aec.initialize(hidden_size, visible_size)\n",
    "J = lambda theta: aec.autoencoder_loss(theta, visible_size, hidden_size, lambda_, \\\n",
    "                                       sparsity_param, beta, data[:10000])\n",
    "optimal = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, \\\n",
    "                                  options={'disp' : True, 'maxiter' : 2000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = optimal['x']\n",
    "w = W[:192*75].reshape(192, 75)\n",
    "w = w.T\n",
    "reload(dl)\n",
    "dl.display_layer(w[:64], 'layer4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полученные фильтры** \n",
    "\n",
    "|$\\lambda = 10^{-4}, \\rho = 10^{-2}, \\beta = 3$|$\\lambda = 10^{-4}, \\rho = 10^{-2}, \\beta = 0.1$|\n",
    "| :--------------------------:|:---------------------------:|\n",
    "|<img src=\"layer.png\", width=250, align=left>  |<img src=\"layer2.png\", width=250, align=left> |\n",
    "|$\\lambda = 10^{-4}, \\rho = 0.1, \\beta = 3$    |$\\lambda = 5*10^{-4}, \\rho = 10^{-2}, \\beta = 3$   |\n",
    "|<img src=\"layer3.png\", width=250, align=left> |<img src=\"layer4.png\", width=250, align=left> |\n",
    "Видно, что при уменьшениии $\\beta$ фильтры серееют. При увеличении $\\rho$ портятся фильтры границ. При увеличении $\\lambda$ нейроны почти никогда не активируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Обучение классификатора на данных сокращённой размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение параметров однослойного автокодировщика**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(aec)\n",
    "hidden_size = np.array([75])\n",
    "visible_size = 192\n",
    "lambda_ = 1e-4\n",
    "sparsity_param = 1e-2\n",
    "beta = 3\n",
    "theta = aec.initialize(hidden_size, visible_size)\n",
    "J = lambda theta: aec.autoencoder_loss(theta, visible_size, hidden_size, lambda_, \\\n",
    "                                       sparsity_param, beta, data[:10000])\n",
    "optimal = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, \\\n",
    "                                  options={'disp' : True, 'maxiter' : 2000})\n",
    "theta = optimal['x']\n",
    "np.savez('theta.npz', theta=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделение признаков автокодировщиком**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./data/train.pk', 'rb')\n",
    "train = pickle.load(f)\n",
    "f.close()\n",
    "f = open('./data/test.pk', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки для изображения из автокодировщика генерируются следующим образом: из изображения с равномерным шагом генерируются патчи фиксированного размера, после чего каждый патч пропускается до среднего слоя обученной нейросети(получая новые признаки), все признаки конкатенируются и образуют итоговые признаки для изображения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoencoder_features(theta, hidden_size, visible_size, data, step=8, patch_size=8):\n",
    "    features = []\n",
    "    pic_cnt = data.shape[0]\n",
    "    pic_size = int(np.sqrt(data.shape[1] // 3))\n",
    "    data = data.reshape((pic_cnt, pic_size, pic_size, 3))\n",
    "    layer_number = (2 + hidden_size.size) // 2\n",
    "    for i in range(0, pic_size-patch_size+1, step):\n",
    "        for j in range(0, pic_size-patch_size+1, step):\n",
    "            data_new = data[:, i:i+patch_size, j:j+patch_size, :].reshape(pic_cnt, patch_size*patch_size*3)\n",
    "            features.append(aec.autoencoder_transform(theta, visible_size, hidden_size, \\\n",
    "                                                      layer_number, data_new))\n",
    "    return np.hstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение рандомизированных деревьев и алгоритма опорных векторов. В качестве признаков используются значения интенсивностей цветных каналов изображений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train['X']\n",
    "y = train['y'].ravel()\n",
    "trees = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=3)\n",
    "svm = sklearn.svm.LinearSVC()\n",
    "trees.fit(X, y)\n",
    "svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43675\n",
      "0.28375\n"
     ]
    }
   ],
   "source": [
    "X_test = test['X']\n",
    "y_test = test['y'].ravel()\n",
    "print(np.mean(trees.predict(X_test) == y_test))\n",
    "print(np.mean(svm.predict(X_test) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение рандомизированных деревьев и алгоритма опорных векторов. Признаки получены с помощью однослойного автокодировщика. Шаг =  8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data\n",
    "f = open('./data/train.pk', 'rb')\n",
    "train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = autoencoder_features(theta, hidden_size, visible_size, train['X'], step=8, patch_size=8)\n",
    "y2 = train['y'].ravel()\n",
    "trees2 = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=3)\n",
    "svm2 = sklearn.svm.LinearSVC()\n",
    "trees2.fit(X2, y2)\n",
    "svm2.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X2\n",
    "del y2\n",
    "del train\n",
    "f = open('./data/test.pk', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43275\n",
      "0.427875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "X_test = test['X']\n",
    "y_test = test['y'].ravel()\n",
    "sum_tree = 0\n",
    "sum_svm = 0\n",
    "for i in range(0, X_test.shape[0], 500):\n",
    "    X_test2 = autoencoder_features(theta, hidden_size, visible_size, X_test[i:i+500], step=8, patch_size=8)\n",
    "    y_test2 = test['y'][i:i+500].ravel()\n",
    "    sum_tree += (np.sum(trees2.predict(X_test2) == y_test2))\n",
    "    sum_svm += (np.sum(svm2.predict(X_test2) == y_test2))\n",
    "print(sum_tree/X_test.shape[0])\n",
    "print(sum_svm/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test\n",
    "del y_test\n",
    "del X_test2\n",
    "del y_test2\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение рандомизированных деревьев и алгоритма опорных векторов. Признаки получены с помощью однослойного автокодировщика. Шаг = 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./data/train.pk', 'rb')\n",
    "train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = autoencoder_features(theta, hidden_size, visible_size, train['X'], step=4, patch_size=8)\n",
    "y3 = train['y'].ravel()\n",
    "trees3 = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=3)\n",
    "svm3 = sklearn.svm.LinearSVC()\n",
    "trees3.fit(X3, y3)\n",
    "svm3.fit(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X3\n",
    "del y3\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./data/test.pk', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.433\n",
      "0.475875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "X_test = test['X']\n",
    "y_test = test['y'].ravel()\n",
    "sum_tree3 = 0\n",
    "sum_svm3 = 0\n",
    "for i in range(0, X_test.shape[0], 500):\n",
    "    X_test3 = autoencoder_features(theta, hidden_size, visible_size, X_test[i:i+500], step=4, patch_size=8)\n",
    "    y_test3 = test['y'][i:i+500].ravel()\n",
    "    sum_tree3 += (np.sum(trees3.predict(X_test3) == y_test3))\n",
    "    sum_svm3 += (np.sum(svm3.predict(X_test3) == y_test3))\n",
    "print(sum_tree3/X_test.shape[0])\n",
    "print(sum_svm3/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test\n",
    "del y_test\n",
    "del X_test3\n",
    "del y_test3\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Обучение параметров трёхслойного автокодировщика**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(aec)\n",
    "hidden_size = np.array([100, 75, 100])\n",
    "visible_size = 192\n",
    "lambda_ = 1e-6\n",
    "sparsity_param = 1e-2\n",
    "beta = 3\n",
    "theta = aec.initialize(hidden_size, visible_size)\n",
    "J = lambda theta: aec.autoencoder_loss(theta, visible_size, hidden_size, lambda_, \\\n",
    "                                       sparsity_param, beta, data[:10000])\n",
    "optimal = scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, \\\n",
    "                                  options={'disp' : True, 'maxiter' : 2000})\n",
    "theta = optimal['x']\n",
    "np.savez('theta_3.npz', theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data\n",
    "f = open('./data/train.pk', 'rb')\n",
    "train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение рандомизированных деревьев и алгоритма опорных векторов. Признаки получены с помощью трёхслойного автокодировщика. Шаг = 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4 = autoencoder_features(theta, hidden_size, visible_size, train['X'], step=8, patch_size=8)\n",
    "y4 = train['y'].ravel()\n",
    "trees4 = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=3)\n",
    "svm4 = sklearn.svm.LinearSVC()\n",
    "trees4.fit(X4, y4)\n",
    "svm4.fit(X4, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X4\n",
    "del y4\n",
    "del train\n",
    "f = open('./data/test.pk', 'rb')\n",
    "test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.399625\n",
      "0.342125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\prak\\prak\\lab5\\autoencoder.py:85: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "X_test = test['X']\n",
    "y_test = test['y'].ravel()\n",
    "sum_tree4 = 0\n",
    "sum_svm4 = 0\n",
    "for i in range(0, X_test.shape[0], 500):\n",
    "    X_test4 = autoencoder_features(theta, hidden_size, visible_size, X_test[i:i+500], step=8, patch_size=8)\n",
    "    y_test4 = test['y'][i:i+500].ravel()\n",
    "    sum_tree4 += (np.sum(trees4.predict(X_test4) == y_test4))\n",
    "    sum_svm4 += (np.sum(svm4.predict(X_test4) == y_test4))\n",
    "print(sum_tree4/X_test.shape[0])\n",
    "print(sum_svm4/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test\n",
    "del y_test\n",
    "del X_test4\n",
    "del y_test4\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = theta\n",
    "w = W[:192*100].reshape(192, 100)\n",
    "w = w.T\n",
    "reload(dl)\n",
    "dl.display_layer(w[:64], 'check.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Результаты экспериментов **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Признаки|Random Forest| SVM|\n",
    "|:---:|:---:|:---:|\n",
    "|Интенсивности цветовых каналов|43.6%|28%|\n",
    "|Однослойный автокодировщик с шагом 8|43.2%|42%|\n",
    "|Однослойный автокодировщик с шагом 4|43.3%|47%|\n",
    "|Трёхслойный автокодировщик с шагом 8|39%|34%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Выводы **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По проделанным экспериментам можно заключить, что в случае с алгоритмом опорных векторов признаки, полученные однослойным автокодировщиком, улучшают работу агоритма с 28% точности до 42% - 47%. А в случае с рандомизированными деревьями точность почти не изменяется. Трёхслойный автокодировщик генерирует более плохие признаки, чем однослойный, что несоответствует теоретическим предположениям. Скорее всего, это связано с неудачной настройкой параметров трёхслойного автокодиовщика."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
